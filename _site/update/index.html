<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <title>
      
        Mid-Term Project Update - Visual Relationship Detection
      
    </title>
    <meta name="viewport" content="width=device-width">
    <link rel="stylesheet" href="/assets/css/normalize.css">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
      <script src="/assets/js/respond.min.js"></script>
    <![endif]-->

  </head>
  <body>
    <div class="container">
      <header role="banner">
        <div class="wrap">
          
          <h1 class="site-title"><a class="title-link" href="/">Visual Relationship Detection</a></h1>
        </div>
      </header>

      <div class="wrap content">

        <aside>
  <p class="intro">Term Project for CS 6476 - Computer Vision </p>
  <br />
  <p> Arvind Krishnakumar, Asawaree Bhide, Divyansh Roy, Shubhangi Upasani </p>
  <a class="skip-link visuallyhidden focusable" href="#main">Skip to Main Content</a>
  <nav class="sidebar-nav" role="navigation">
    <ul>
      
        
          <li><a href="/./">Home</a></li>
        
      
        
          <li><a href="/problem_statement/">Problem Statement</a></li>
        
      
        
          <li><a href="/proposal/">Proposal</a></li>
        
      
        
          <li><a href="/update/">Mid-Term Project Update</a></li>
        
      
        
          <li><a href="/references/">References</a></li>
        
      
        
          <li><a href="https://github.com/akrishna77/visual-relationships">houghlepuff Repo</a></li>
        
      
    </ul>
  </nav>
</aside>

        <section id="main" class="main-content" role="main">
          <h2 class="page-title">Mid-Term Project Update</h2>

<h3 id="abstract">Abstract</h3>

<p>The goal of our project is to identify specific visual relationships in images, to make progress towards genuine scene understanding. We detect object-object relationships in an image based on a textual description given by the user; a subject, object and predicate triplet (for instance, ‘plate next to pizza’). For this, we plan to optimize a triplet loss function using a pre-trained convolutional neural network (VGG16).</p>

<h3 id="teaser-figure">Teaser Figure</h3>

<center><img src="../assets/img/pizza_plate.png" width="450px" alt="" /></center>

<h3 id="introduction">Introduction</h3>

<p>The existing implementations for visual relationship detection output all possible object-object relationships when fed an image. We intend to detect specific object pairs and a relationship predicate between them that match the given textual input while at the same time try and minimize the number of erroneous pairs our model outputs.</p>

<h3 id="approach">Approach</h3>

<p>At this stage in the project, we have explored and obtained results for a few of our proposed modules.</p>

<ul>
  <li>
    <p>Query Parsing - The module is used to extract the &lt;subject, predicate, object&gt; from the input query. After exploring three different approaches to this problem, Spacy, NLTK and Stanford’s CoreNLP, we evaluated their results and determined that Spacy had higher precision in detecting &lt;s,p,o&gt; and would work best for our particular use-case. <a href="https://github.com/akrishna77/CS6476-TermProject/blob/master/VisRel%20%2B%20All%20Pairs%20Detection.ipynb">[Code]</a></p>
  </li>
  <li>
    <p>Object Detection - This module is used to detect &lt;subject, object&gt; pairs, corresponding to the input query. State-of-the-art object detection models currently have really high accuracy, so we used the pretrained weights from a Resnet50 Faster R-CNN model and obtained bounding boxes and categories for objects in the input image. We then tailored our results to return all pairs relevant to the &lt;s,o&gt; in our input query. <a href="https://github.com/akrishna77/CS6476-TermProject/blob/master/VisRel%20%2B%20All%20Pairs%20Detection.ipynb">[Code]</a></p>

    <p>For example, for the query &lt;person,car&gt; against the following image, we would return 24 images (every combination of the 6 persons and 4 cars detected in the image):</p>
  </li>
</ul>

<center><img src="../assets/img/bbox1.png" width="450px" alt="" /></center>
<center><img src="../assets/img/bbox2.png" width="450px" alt="" /></center>
<center><img src="../assets/img/bbox3.png" width="450px" alt="" /></center>
<center><img src="../assets/img/bbox4.png" width="450px" alt="" /></center>

<ul>
  <li>
    <p>Data Exploration - We did some analysis on the VRD images’ annotations to learn more information about the nature of the dataset and common subjects, predicates and objects, that had more samples. <a href="https://github.com/akrishna77/CS6476-TermProject/blob/master/DataExploration.ipynb">[Code]</a></p>
  </li>
  <li>
    <p>Data Collection - Using the VR dataset, we determined positive and negative examples for each of the 70 relationship classes (including ‘on’, ‘next to’, ‘behind’, ‘under’). We accomplished this using heuristics to distinguish negative samples from positive samples against the input image. This mapping is then used to train our model with a triplet loss function that tries to minimize the distance with a positive example while maximizing distance from a negative example. <a href="https://github.com/akrishna77/CS6476-TermProject/blob/master/Triplets_Dataset_Creation.ipynb">[Code]</a></p>

    <p>A challenge here is determining a hard negative sample for every predicate. For instance, given an image with a man ‘next to’ a woman, a good negative image would be one with a man ‘behind’ a woman. A good positive example would be another image with a ‘next to’ relationship between a man and a woman.</p>
  </li>
  <li>
    <p>Training the Model - We have also written baseline code to implement model training using the triplet loss function in PyTorch. This is currently under progress. <a href="https://github.com/akrishna77/CS6476-TermProject/blob/master/TripletLossTrain.ipynb">[Code]</a></p>
  </li>
</ul>

<h3 id="experiments-and-results">Experiments and Results</h3>

<p>We have collected 70070 positive and negative pairs for the images in the VRD dataset.</p>

<p>Here are some examples:</p>

<center><img src="../assets/img/img1.png" width="450px" alt="" /></center>
<p>Here, the input description given is ‘tree behind woman’. The image on the left clearly is a good positive example of this relationship (behind) between the subject (tree) and object (woman). The image on the right is a good negative example because it is close enough to the input (same subject and object) but very different in terms of the relationship (the predicate ‘left of’ instead of ‘behind’).</p>
<center><img src="../assets/img/img2.png" width="450px" alt="" /></center>
<center><img src="../assets/img/img3.png" width="450px" alt="" /></center>
<center><img src="../assets/img/img4.png" width="450px" alt="" /></center>
<center><img src="../assets/img/img5.png" width="450px" alt="" /></center>
<center><img src="../assets/img/img6.png" width="450px" alt="" /></center>
<center><img src="../assets/img/img7.png" width="450px" alt="" /></center>
<center><img src="../assets/img/img8.png" width="450px" alt="" /></center>
<center><img src="../assets/img/img9.png" width="450px" alt="" /></center>
<center><img src="../assets/img/img10.png" width="450px" alt="" /></center>
<center><img src="../assets/img/img11.png" width="450px" alt="" /></center>

<p>A challenge we see ihere is that certain pairs of predicates are semantically very similar (for instance, ‘above’ and ‘over’ or ‘beside’ and ‘by’ could be used interchangeably in most cases). In this case, the negative sample obtained may actually be a good positive sample as a similar relationship is observed by the objects in the image. Due to the scope of the project, we will be limiting predicates to those where our negative sampling works well.</p>

<p>As of now, we have prepared the dataset and implemented the language and vision sub-modules we would be using. We will now start working on training the model with the collected positive and negative examples for each predicate.</p>

<h3 id="conclusion-and-future-plan-of-action">Conclusion and Future Plan of Action</h3>

<p>The next step is to train the model using our prepared dataset. The goal is to train a model that will be able to differentiate between  different kinds of relationships and match them to the queried predicate. We use triplet loss to better train the model and use lesser data at the same time. During testing, we would pass an input image and query to the model and expect it to determine the specified relationship.</p>

<p>We plan to make the pipeline adaptable to more natural textual inputs (full sentences and not just &lt;subject,predicate,object&gt;), as an extension to the projet, if time permits. We would also like to include heuristics-based pair filtering in the Object Detection module, as an optimization.</p>


        </section>

      </div><!-- /.wrap content -->

      <footer role="contentinfo">
        <div class="wrap">
          <p>This project is maintained by <a href="http://github.com/akrishna77">Arvind</a>.</p>

          <p>Hosted on <a href="http://pages.github.com/">GitHub Pages</a>.</p>
        </div><!--/.wrap -->
      </footer>
    </div> <!-- /.container -->
    
    </body>
</html>
