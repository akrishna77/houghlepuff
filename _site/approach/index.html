<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <title>
      
        Approach - Visual Relationship Detection
      
    </title>
    <meta name="viewport" content="width=device-width">
    <link rel="stylesheet" href="/assets/css/normalize.css">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
      <script src="/assets/js/respond.min.js"></script>
    <![endif]-->

  </head>
  <body>
    <div class="container">
      <header role="banner">
        <div class="wrap">
          
          <h1 class="site-title"><a class="title-link" href="/">Visual Relationship Detection</a></h1>
        </div>
      </header>

      <div class="wrap content">

        <aside>
  <p class="intro">Term Project for CS 6476 - Computer Vision </p>
  <br />
  <p> Arvind Krishnakumar, Asawaree Bhide, Divyansh Roy, Shubhangi Upasani </p>
  <a class="skip-link visuallyhidden focusable" href="#main">Skip to Main Content</a>
  <nav class="sidebar-nav" role="navigation">
    <ul>
      
        
          <li><a href="/./">Home</a></li>
        
      
        
          <li><a href="/problem_statement/">Problem Statement</a></li>
        
      
        
          <li><a href="/proposal/">Proposal</a></li>
        
      
        
          <li><a href="/update/">Mid-Term Project Update</a></li>
        
      
        
          <li><a href="/references/">References</a></li>
        
      
        
          <li><a href="https://github.com/akrishna77/visual-relationships">houghlepuff Repo</a></li>
        
      
    </ul>
  </nav>
</aside>

        <section id="main" class="main-content" role="main">
          <h2 class="page-title">Approach</h2>

<p>The existing implementations for visual relationship detection output all possible object-object relationships when fed an image. We intend to detect specific object pairs and a relationship predicate between them that match the given textual input while at the same time try and minimize the number of erroneous pairs our model outputs. The model to achieve the same is described in the subsequent sections.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1.1. The first step would be to identify a &lt;subject, predicate, object&gt; triplet from the given text input. We will be using the Stanford NLP toolkit/spacy for this task.

1.2. We would then find all possible object pairs in the image. Current state-of-the-art models for object detection achieve impressive performance by leveraging deep convolutional neural networks. We plan to use the Fast RCNN pipeline for this module. 

1.3 After finding all possible object pairs, we find object pairs that correspond to the &lt;s,o&gt; detected in the triplet obtained from step 1.1. 

1.4 We also introduce a thresholded intra-bounding box distance filter to discard pairs of &lt;s,o&gt; that are spatially distant in the image. This is to keep in line with our limited set of spatial predicates.
</code></pre></div></div>

<p>In this project, we propose a few-shot learning based approach to learn the predicate relationship in images. We do this by using a pre-trained convolutional neural network (VGG16) and optimizing a triplet-loss function. Triplet loss is a loss function for neural networks where a baseline (anchor) input is compared to a positive input and a negative input. The distance from the baseline input to the positive input is minimized, and the distance from the baseline input to the negative input is maximized. The input to this model would be an image of our &lt;s,o&gt; pair. The output of this model would be class probabilities for our limited set of predicates. Due to computational limitations and time constraints, we will be using a limited set of spatial predicates to test our approach.</p>

<h2 id="tasks">Tasks:</h2>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Object Detection: Returning bounding boxes around entities in the image that look like the ‘subject’ and the ‘object’ from the input description.

Object Pair Filtering: Filtering out object pairs in the image that do not have a semantically meaningful relationship between them (spatially distant objects).

Neural Network Model: Based on the positions of relevant bounding boxes with respect to each other, detect the probability of the relationship corresponding to the input predicate.
</code></pre></div></div>


        </section>

      </div><!-- /.wrap content -->

      <footer role="contentinfo">
        <div class="wrap">
          <p>This project is maintained by <a href="http://github.com/akrishna77">Arvind</a>.</p>

          <p>Hosted on <a href="http://pages.github.com/">GitHub Pages</a>.</p>
        </div><!--/.wrap -->
      </footer>
    </div> <!-- /.container -->
    
    </body>
</html>
