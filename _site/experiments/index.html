<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <title>
      
        Experiments - Visual Relationship Detection
      
    </title>
    <meta name="viewport" content="width=device-width">
    <link rel="stylesheet" href="/assets/css/normalize.css">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
      <script src="/assets/js/respond.min.js"></script>
    <![endif]-->

  </head>
  <body>
    <div class="container">
      <header role="banner">
        <div class="wrap">
          
          <h1 class="site-title"><a class="title-link" href="/">Visual Relationship Detection</a></h1>
        </div>
      </header>

      <div class="wrap content">

        <aside>
  <p class="intro">Term Project for CS 6476 - Computer Vision </p>
  <br />
  <p> Authors: Arvind Krishnakumar, Asawaree Bhide, Divyansh Roy, Shubhangi Upasani </p>
  <a class="skip-link visuallyhidden focusable" href="#main">Skip to Main Content</a>
  <nav class="sidebar-nav" role="navigation">
    <ul>
      
        
          <li><a href="/./">Home</a></li>
        
      
        
          <li><a href="/proposal/">Proposal</a></li>
        
      
        
          <li><a href="https://github.com/akrishna77/houghlepuff">houghlepuff Repo</a></li>
        
      
    </ul>
  </nav>
</aside>

        <section id="main" class="main-content" role="main">
          <h2 class="page-title">Experiments</h2>

<p>For implementation, we use Python 3.x version and keras or PyTorch framework with required libraries. 
Dataset used- We use the Visual Relationship Dataset which has 5000 images with 100 object categories and 70 predicates. In total, it has 37,993 relationships and 6,672 relationship types. The relationships have been classified into many categories, some of which are action, preposition, comparative, verb etc.  Some examples of relationships included in the dataset are  ‘person kick ball’, ‘person on top of ramp’, ‘motorcycle with wheel’, ‘man riding horse’ etc.</p>

<p>We may use bits of existing code from the following projects- 
https://github.com/pjreddie/darknet/wiki/YOLO:-Real-Time-Object-Detection (for detecting object pairs)</p>

<p>https://github.com/matterport/Mask_RCNN (for detecting object pairs)</p>

<p>https://github.com/yikang-li/vg_cleansing (for creating the language module)</p>

<p>https://github.com/jz462/Large-Scale-VRD.pytorch (for visual relationship understanding)</p>

<p>https://github.com/doubledaibo/drnet_cvpr2017 (for visual relationship detection[2])</p>

<p>https://github.com/GriffinLiang/vrd-dsr (for visual relationship detection[3])</p>

<p>The existing implementations for visual relationship detection output all possible object-object relationships when fed a test image. We intend to detect specific object pairs and a relationship predicate between them that match the given textual input while at the same time try and minimize the number of erroneous pairs our model outputs.</p>

<p>A specific case we want to handle is as follows:
Given an input ‘man holding pen’ and an image where there is a man and a pen lying on the notebook next to him, our system should not display bounding boxes around the man or the pen because there is no ‘holding’ relationship. As another example, for the image below, the input ‘man on horse’ should correctly ignore the man walking in front of the horse. This relationship should be discarded during the filtering stage.</p>


        </section>

      </div><!-- /.wrap content -->

      <footer role="contentinfo">
        <div class="wrap">
          <p>This project is maintained by <a href="http://github.com/akrishna77">Arvind</a>.</p>

          <p>Hosted on <a href="http://pages.github.com/">GitHub Pages</a>.</p>
        </div><!--/.wrap -->
      </footer>
    </div> <!-- /.container -->
    
    </body>
</html>
